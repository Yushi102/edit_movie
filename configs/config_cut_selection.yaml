# Cut Selection Model Configuration
# Stage 1: Select which parts of the video to keep (cut selection only)
# Uses audio + visual features only, no track information

# Data paths
train_data: preprocessed_data/train_sequences_cut_selection.npz
val_data: preprocessed_data/val_sequences_cut_selection.npz
features_dir: data/processed/source_features  # Not used (data already integrated in NPZ)

# Multimodal settings
enable_multimodal: true
fusion_type: gated
use_modality_attention_mask: true
# Note: Track features are used during training as ground truth,
# but will be set to zero during inference for cut selection

# Feature dimensions
audio_features: 215  # 4 base + 192 speaker_emb + 16 emotion + 2 text/telop + 1 speaker_id
# 内訳:
#   基本音声特徴（4次元）:
#     - audio_energy_rms: 1
#     - audio_is_speaking: 1
#     - silence_duration_ms: 1
#     - speaker_id: 1
#   話者埋め込み（192次元）:
#     - speaker_emb_0~191: 192
#   感情表現特徴（16次元）:
#     - pitch_f0: 1 (基本周波数)
#     - pitch_std: 1 (ピッチの変動)
#     - spectral_centroid: 1 (音色の明るさ)
#     - zcr: 1 (ゼロ交差率)
#     - mfcc_0~12: 13 (メル周波数ケプストラム係数)
#   テキスト・テロップ（3次元）:
#     - text_is_active: 1
#     - telop_active: 1
#     - (text_word, telop_textは文字列なので特徴量には含まれない)
visual_features: 522  # 10 scalar + 512 CLIP
track_features: 240  # 20 tracks × 12 parameters (active, asset_id, scale, pos_x, pos_y, anchor_x, anchor_y, rotation, crop_l, crop_r, crop_t, crop_b)

# Model architecture
d_model: 256
nhead: 8
num_encoder_layers: 6
dim_feedforward: 1024
dropout: 0.15  # Increased for better regularization (was 0.1)
num_tracks: 20
max_asset_classes: 250  # Support up to 250 different character assets (max observed: 212)
# Character Selection Strategy:
#   Asset ID is disabled (set to 1 for compatibility)
#   Character images are selected based on speaker identification:
#   - Speaker embedding model (256D) extracts voice characteristics
#   - Model learns: "this speaker embedding → this character image"
#   - Supports 100+ unique characters
#   - Automatic speaker clustering per video
# 
# How it works:
#   1. Training: 
#      - Extract 256D speaker embeddings using pyannote.audio
#      - Cluster speakers within each video (typically 5 speakers/video)
#      - Model learns correlation between speaker embeddings and character images
#   2. Inference: 
#      - Extract speaker embeddings from input audio
#      - Model predicts which character image to show based on embeddings
#   3. Example: Speaker A's embedding → Character A image, Speaker B → Character B
# 
# Benefits:
#   - Robust speaker discrimination with 256D embeddings
#   - Automatic character matching based on voice identity
#   - No manual speaker labeling required
#   - Works with 100+ characters across all videos
#   - Generalizes to new speakers with similar voice characteristics

# Training hyperparameters
batch_size: 16  # Increased for GPU (was 4 for CPU)
num_epochs: 50  # Increased for full training (was 20)
learning_rate: 0.0001  # Reduced from 0.001 for more stable learning
weight_decay: 0.0001  # Increased for better regularization (was 0.00001)
grad_clip: 1.0

# Optimizer and scheduler
optimizer: adam
scheduler: cosine
warmup_epochs: 2
min_lr: 0.000001

# Loss weights
# Balanced approach: prevent overfitting while detecting active samples
auto_balance_weights: false  # Don't auto-balance - use manual weights
use_focal_loss: true  # Use Focal Loss for better learning on hard examples
focal_alpha: 0.70  # Balanced weight for active class (0.70 = 採用を見逃すと2.3倍のペナルティ)
                   # alpha > 0.5 means: missing active samples is more costly
                   # 0.5 = balanced, 0.70 = 2.3x penalty for FN, 0.75 = 3x penalty
                   # Reduced from 0.75 to prevent overfitting
focal_gamma: 2.0  # Focus on hard examples

# Temporal smoothness (prevents chattering/flickering)
tv_weight: 0.05  # Total Variation loss weight (0.01-0.5)
                 # Balanced between classification accuracy and smoothness

active_weight: 100.0  # Focus only on cut selection (used if auto_balance_weights=false)
asset_weight: 0.0  # Disable - will be handled in stage 2
scale_weight: 0.0  # Disable - will be handled in stage 2
position_weight: 0.0  # Disable - will be handled in stage 2
rotation_weight: 0.0  # Disable - will be handled in stage 2
crop_weight: 0.0  # Disable - will be handled in stage 2
ignore_inactive: true
label_smoothing: 0.0  # Disabled for clearer classification (was 0.1)

# Checkpointing
checkpoint_dir: checkpoints_cut_selection
save_every: 5
early_stopping_patience: 20  # Increased from 15 to allow more training

# Data loading
num_workers: 0  # Set to 0 for Windows compatibility (multiprocessing issues)
                # On Linux/Mac, can use 4-8 for faster data loading

# Device
cpu: false  # Use GPU (CUDA) if available

# Performance optimization (GPU)
use_amp: true  # Automatic Mixed Precision (1.5-2x faster on GPU, 50% less VRAM)
gradient_accumulation_steps: 1  # Increase if GPU memory is limited (effective_batch = batch_size * N)

# Feature alignment settings
alignment_tolerance: 0.05
max_interpolation_ratio: 0.5
max_gap_seconds: 5.0

# Memory optimization
use_fp16_visual: true
lazy_loading: true
cache_aligned_features: true

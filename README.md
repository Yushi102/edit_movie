# 動画編集AI - 自動カット選択システム

動画から自動的に**最適なカット位置**を予測し、Premiere Pro用のXMLを生成するAIシステムです。

**想定用途**: 10分程度の動画を約2分（90秒〜150秒）のハイライト動画に自動編集

## 🎯 現在の開発フォーカス

**本プロジェクトは現在、カット選択（Cut Selection）に特化して開発中です。**

- ✅ **カット選択モデル**: 高精度で動作中（F1スコア: 0.5630）
- ⚠️ **グラフィック配置・テロップ生成**: 精度が低いため今後の課題
  - 現在のマルチモーダルモデル（音声・映像・トラック統合）は、グラフィック配置やテロップ生成の精度が実用レベルに達していません
  - カット選択に集中することで、より高品質な自動編集を実現します
  - グラフィック・テロップ機能は将来的に改善予定です

## 🎯 機能

### 現在実装済み（カット選択）
- **自動カット検出**: AIが最適なカット位置を予測（F1スコア: 0.5630）
- **音声同期カット**: 映像と音声を同じ位置で自動カット
- **クリップフィルタリング**: 短すぎるクリップの除外、ギャップ結合、優先順位付け
- **Premiere Pro連携**: 生成されたXMLをそのままPremiere Proで開ける
- **リアルタイム学習可視化**: 6つのグラフで学習状況を監視

### 将来的に実装予定の機能（精度改善後）
- **グラフィック配置の自動化**: キャラクター立ち絵の配置・スケール・位置調整
  - 現在のモデルでは精度が低く実用レベルに達していません
  - カット選択の精度向上を優先し、その後に取り組みます
- **AI字幕生成**: 音声認識（Whisper）と感情検出による自動字幕生成
- **テロップ自動配置**: OCRで検出したテロップのXML出力
  - Base64エンコード形式の解析が必要
- **動的な解像度対応**: 入力動画に応じた自動シーケンス設定

## 📁 プロジェクト構造

```
xmlai/
├── src/                          # ソースコード
│   ├── data_preparation/         # データ準備
│   ├── model/                    # モデル定義
│   ├── training/                 # 学習
│   ├── inference/                # 推論
│   └── utils/                    # ユーティリティ
├── scripts/                      # 補助スクリプト
├── tests/                        # テストコード
├── configs/                      # 設定ファイル
├── docs/                         # ドキュメント
├── data/                         # データ（.gitignoreで除外）
├── checkpoints/                  # 学習済みモデル（.gitignoreで除外）
├── preprocessed_data/            # 前処理済みデータ（.gitignoreで除外）
├── outputs/                      # 出力ファイル
├── archive/                      # アーカイブ（.gitignoreで除外）
└── backups/                      # バックアップ（.gitignoreで除外）
```

## 🚀 クイックスタート

### 必要な環境
- **OS**: Windows（バッチファイルを使用）
  - Mac/Linuxの場合は、Pythonコマンドを直接実行してください
- **Python**: 3.8以上
- **GPU**: CUDA対応GPU（推奨）
- **Premiere Pro**: XML読み込み用
- **⚠️ 重要**: プロジェクトパスに日本語などの非ASCII文字を含めないでください
  - MediaPipeが正常に動作しない可能性があります
  - 推奨: `C:\projects\xmlai` のようなASCII文字のみのパス

### インストール
```bash
pip install -r requirements.txt
```

**注意**: `requirements.txt`には動作確認済みのバージョンがコメントで記載されています。最小バージョン要件を満たしていれば、より新しいバージョンでも動作する可能性があります。

### 新しい動画を自動編集

**方法1: バッチファイルを使う（推奨）**
```bash
run_inference.bat "path\to\your_video.mp4"
```

**方法2: 手動で実行**
```bash
# 推論実行
python -m src.inference.inference_pipeline "your_video.mp4" outputs/inference_results/output.xml

# Premiere Proで output.xml を開く
```

詳しくは [QUICK_START.md](docs/QUICK_START.md) を参照してください。

## 📚 ドキュメント

### 基本ガイド
- [プロジェクト全体の流れ](docs/guides/PROJECT_WORKFLOW_GUIDE.md)
- [必要なファイル一覧](docs/guides/REQUIRED_FILES_BY_PHASE.md)
- [音声カット & テロップ変換](docs/summaries/AUDIO_CUT_AND_TELOP_GRAPHICS_SUMMARY.md)

## 🔧 開発

### データ準備
```bash
# バッチファイルで一括実行（推奨）
run_data_preparation.bat

# または手動で実行
python -m src.data_preparation.premiere_xml_parser
python -m src.data_preparation.extract_video_features_parallel
python -m src.data_preparation.data_preprocessing
```

### 学習

**カット選択モデルのトレーニング:**
```bash
# 1. データ準備（動画単位で分割）
python scripts/create_cut_selection_data.py

# 2. トレーニング実行（可視化付き）
train_cut_selection.bat

# 3. 学習状況の確認
# ブラウザで checkpoints_cut_selection/view_training.html を開く
# 2秒ごとに自動更新されるグラフで学習の様子をリアルタイム確認
```

**可視化される情報:**
- 損失関数（Train/Val Loss）
- 損失の内訳（CE Loss vs TV Loss）
- 分類性能（Accuracy & F1 Score）
- Precision, Recall, Specificity
- 最適閾値の推移
- 予測の採用/不採用割合

**データ分割の特徴:**
- 動画単位で学習/検証に分割（データリーク防止）
- 同じ動画のシーケンスは必ず同じセットに配置
- より厳密な汎化性能の評価が可能

### テスト
```bash
# ユニットテスト
pytest tests/unit/

# 統合テスト
pytest tests/integration/
```

## 📊 性能

### カット選択モデル（Cut Selection Model）
- **学習データ**: 94本の動画から218,693フレーム
  - 68本の動画を301シーケンスに分割（シーケンス長1000フレーム、オーバーラップ500）
  - **動画単位分割**: 学習54動画（210シーケンス）、検証14動画（91シーケンス）
  - データリーク防止のため、同じ動画のシーケンスは必ず同じセット（学習 or 検証）に配置
- **採用率**: 全体26.93%（学習28.19%、検証12.14%）
- **想定入力**: 10分程度の動画
- **出力**: 約2分（90秒〜150秒）のハイライト動画
- **学習時間**: 50エポック（約1-2時間、GPU使用時）
- **推論時間**: 5~10分/動画（特徴量抽出含む）
- **カット数**: 約8〜12個のクリップ（最小3秒、ギャップ結合・優先順位付け後）
- **モデル性能**: 
  - Best F1スコア: 0.5630（Epoch 33、focal_alpha=0.75）
  - 最適閾値: -0.200（学習時に自動計算）
  - Focal Loss使用（alpha=0.70、採用見逃しに2.3倍のペナルティ）

## ⚠️ 既知の問題点・改善点

### 現在の問題点

#### 1. テロップ関連
- **テロップがBase64エンコードで特徴量に含められていない**
  - Premiere ProのBase64エンコード形式のため、テロップの内容や位置情報を学習に活用できていない
  - OCRで検出したテロップ情報が学習データに反映されていない
- **テロップのXML出力未対応**
  - 学習したテロップ情報をXMLに出力する機能が実装されていない
  - 現在はテロップ生成を無効化して対応（`configs/config_telop_generation.yaml`）

#### 2. クリップ生成の問題
- **✅ 解決済み: 過剰なカット問題**
  - 以前は0.1秒ごとの過剰なカットで音声・動画が飛び飛びになっていた
  - **現在の対策**:
    - 最小クリップ継続時間: 3.0秒（3秒未満のクリップは不採用）
    - ギャップ結合: クリップ間のギャップが2秒以内なら自動的に埋めて結合
    - 優先順位付け: モデルの確信度（active確率）が高いクリップを優先選択
    - 合計時間制限: 目標90秒、最大150秒に制限
  - 実装場所: `src/inference/inference_pipeline.py`

#### 3. 編集の自由度
- **単一トラック配置**
  - 現在は1つのトラックに全クリップが時系列順に配置される
  - 複数トラックへの分散配置は未実装（編集の自由度が低い）

#### 4. モデルの確信度の問題 - ✅ 改善済み
- **✅ Focal Lossの導入で確信度が向上**
  - 以前: Active閾値0.29が必要なほど確信度が低かった
  - **現在の対策**:
    - Focal Loss（alpha=0.70、gamma=2.0）を使用
    - 採用見逃し（False Negative）に2.3倍のペナルティ
    - 最適閾値: -0.200（学習時に自動計算）
    - Best F1スコア: 0.5630
  - **効果**: 
    - モデルがactiveクラスに対してより確信を持てるようになった
    - 過学習を防ぎつつF1スコアが向上
    - 動画単位の分割でより厳密な評価が可能に

#### 5. マジックナンバーへの依存（推論時） - ✅ 解決済み
- **✅ 設定ファイル化と自動最適化を実装**
  - 以前: Active閾値0.29がハードコーディングされていた
  - **現在の実装**:
    - 学習時に検証データで最適閾値を自動計算
    - `checkpoints_cut_selection/inference_params.yaml`に保存
    - 推論時に自動的にロード
  - **メリット**:
    - モデルごとに最適な閾値を使用
    - 手動調整が不要
    - 再現性の向上

#### 6. フレーム単位の回帰予測のジッター
- **ScaleやPosition（x, y）の予測が不安定**
  - フレームごとに独立して予測するため、値が微妙に震える（ジッター）
  - 生成された動画で画像がガクガク震える現象が発生
  - **現在の応急処置**:
    - 採用クリップ間の短い不採用クリップを採用として扱う
    - 短い採用クリップで前後に採用クリップがない場合は不採用にする
    - クリップ全体の平均値（`np.mean`）を使用
  - **問題点**:
    - 平均化すると動き（ズームインなど）が消えてしまう
    - プロっぽい滑らかなカメラワークが実現できない
  - **提案**:
    - 移動平均フィルタ（Moving Average）の適用
    - サビツキー・ゴーレイ・フィルタなどで数値を滑らかにする
    - キーフレーム補間を考慮した予測方法の検討

#### 7. XMLパースの複雑さ
- **premiere_xml_parser.pyの制限**
  - 標準的なXML構造のみを想定
  - **対応できない構造**:
    - ネストされたシーケンス（Nested Sequence）
    - マルチカムクリップ
    - 複雑なエフェクトチェーン
  - **問題点**:
    - ネストされた構造内のクリップが無視される
    - 時間計算が正しく行えない可能性
  - **提案**:
    - 再帰的にネストを掘るロジックの実装
    - より堅牢なXMLパーサーの採用（例: OpenTimelineIO）

#### 8. シーケンス設定の未対応
- **Premiere Proのシーケンス設定が反映されない**
  - 解像度、フレームレート、アスペクト比などの設定が固定
  - 縦長動画（1080x1920）に対応しているが、他の解像度は未検証
  - **問題点**:
    - 異なる解像度の動画で正しく動作しない可能性
    - フレームレートの不一致による音ズレの可能性
  - **提案**:
    - 入力動画のメタデータから自動的にシーケンス設定を生成
    - 設定ファイルでシーケンス設定をカスタマイズ可能に

#### 9. Asset ID（素材ID）の汎用性問題 - ✅ 解決済み（話者識別ベース）
- **✅ Asset IDを廃止し、話者識別ベースのキャラクター選択に変更**
  - 以前: ファイル名ごとにasset_id（0-9）を割り当て → 100種類以上の画像で衝突
  - **現在の実装**:
    - Asset IDは使用しない（互換性のため0固定）
    - **話者埋め込み（256次元）でキャラクターを選択**
    - モデルが「この話者 → この立ち絵」を自動学習
  - **仕組み**:
    - 学習時: 
      - pyannote.audioで256次元の話者埋め込みを抽出
      - 動画内で話者を自動クラスタリング（通常5人程度/動画）
      - モデルが話者埋め込みとキャラクター画像の相関を学習
    - 推論時: 
      - 入力音声から話者埋め込みを抽出
      - モデルが埋め込みから適切なキャラクター画像を予測
    - 例: 話者Aの埋め込み → キャラクターA、話者B → キャラクターB
  - **メリット**:
    - 100人以上のキャラクターに対応
    - 手動での話者ラベリング不要
    - 256次元の高次元埋め込みで高精度な話者識別
    - 音声の特徴で自動的にキャラクターを判別
    - 新しい声でも類似した声特徴から汎化可能
    - 「この話者はこの立ち絵」という自然な学習
  - **技術詳細**:
    - 話者埋め込みモデル: speechbrain/spkrec-ecapa-voxceleb（256次元）
    - 発話区間検出: VADベース（0.5秒以上の連続発話）
    - 話者クラスタリング: コサイン類似度ベース（閾値0.7）
    - 音声特徴量: 199次元（4基本 + 1 speaker_id + 192 speaker_emb + 2フラグ）
  - **使用例**:
    - 動画1: 話者Aの声 → モデルが自動的にキャラクターAの立ち絵を選択
    - 動画2: 話者Bの声 → モデルが自動的にキャラクターBの立ち絵を選択
    - 動画3: 話者Cの声 → モデルが自動的にキャラクターCの立ち絵を選択
  - **将来的な拡張**:
    - **感情検出との連携**: 音声から感情を検出して適切な表情を選択
    - **CLIP特徴量の活用**: 立ち絵の視覚的特徴も考慮
    - **話者ダイアライゼーション**: より高度な話者分離アルゴリズム

### コード品質の問題点（深刻）

#### 1. `src/inference/otio_xml_generator.py`の問題（最優先で修正が必要）
- **デッドコード（到達不能コード）の存在**
  - `create_premiere_xml_with_otio`関数の冒頭で`create_premiere_xml_direct`をインポートし、即座にreturn
  - その後の「OTIOを使ってタイムラインを構築する数百行のコード」はすべて実行されない
  - **リスク**: 将来誤ってreturnを消すと、全く異なるロジックが動き出してバグになる
  - **影響**: コードを読む人を混乱させ、メンテナンス性を著しく低下させる

- **正規表現によるXML操作（アンチパターン）**
  - `_post_process_telop_to_graphics_correct`などで`re.sub`を使ってXMLタグを書き換え
  - **問題点**:
    - XMLは構造化データであり、正規表現で安全に扱うのは不可能
    - タグの属性順序や空白が変わるだけでコードが動かなくなる
    - `xml.etree.ElementTree`などのパーサーを使うべき
  - **リスク**: 非常に脆く、Premiere Proのバージョンアップで簡単に壊れる

- **ハードコードされた解像度**
  - XMLテンプレート部分に`<width>1080</width>`, `<height>1920</height>`が固定
  - **問題点**: 横長動画（1920x1080）を入力しても、強制的に縦長の設定でXMLが生成される
  - **影響**: Premiere Proで読み込んだ際にアスペクト比がおかしくなる

#### 2. `src/data_preparation/extract_video_features.py`の問題（パフォーマンス・メモリ）
- **メモリリークのリスク**
  - 動画の全フレーム（またはサンプリング毎）のデータを`records`リストに追加し続ける
  - CLIP特徴量（512次元のfloat）を含むデータをメモリに溜め込み続ける
  - **問題点**: 数十分〜数時間の長い動画を処理するとメモリ不足（OOM）でクラッシュする可能性が高い
  - **提案**: 一定数ごとにCSVに書き出してメモリを解放する（Chunk処理）実装が必要

- **CLIP特徴量の抽出頻度と補間のズレ**
  - 設定で`CLIP_STEP = 1.0`（1秒に1回）だが、全体の特徴量は`TIME_STEP = 0.1`（10FPS）で出力
  - `last_clip_emb`を保持して使い回しているため、「1秒間同じ意味内容が続く」という扱いになる
  - **問題点**: 細かいシーン変化を取り逃がす原因になる

- **例外処理の甘さ**
  - MediaPipeの初期化に失敗した場合、`self.face_mesh = None`となり、顔特徴量はすべて固定値になる
  - **問題点**: 「顔が映っていない」のか「検出器が壊れた」のか区別がつかず、学習データとしてノイズになる可能性

#### 3. `src/utils/feature_alignment.py`の問題（データ整合性）
- **CLIP特徴量のL2正規化**
  - `_align_visual_features`メソッドの最後で、CLIP特徴量をL2ノルムで割って正規化
  - **注意点**: 後段のTransformerモデルが「生の値」を期待しているのか「正規化された値」を期待しているのかによって、正しさが変わる
  - **問題点**: モデル内で`LayerNorm`等が入っているなら重複する処理になる

- **不一致な特徴量次元数のコメント**
  - コメントで「522次元」と書かれているが、実際の計算ロジックと合致しているか不明
  - `num_features`の計算ロジックが手動で調整されており（`# This is 10 features`などのコメント）、特徴量の定義が変わった際にバグが出やすい

#### 4. `src/model/loss.py`の問題（学習の安定性）
- **Lossの重み付けと除算**
  - 回帰損失（Scale, Positionなど）を計算する際、`active_count`（Activeなトラック数）で割って平均化
  - **問題点**:
    - Activeなトラックが極端に少ない（例: 1つだけ）フレームと、多い（例: 20個）フレームで、1トラックあたりのLossの重みが変わる
    - `active_count`が0に近い場合の挙動が学習を不安定にさせる可能性
  - **提案**: 「バッチ全体のActiveな総数」で割るか、Masked Meanの取り方を統一したほうが安定

### 修正の優先順位

#### 緊急（コードの整合性）
1. **`otio_xml_generator.py`の整理**
   - `create_premiere_xml_with_otio`内のデッドコードを削除
   - 正規表現ベースのXML操作をやめて、正しいXMLライブラリ操作に書き換え
   - または`direct_xml_generator.py`に処理を完全に委譲してこのファイルを削除

2. **解像度の動的取得**
   - `width`, `height`をハードコードせず、入力動画（`cv2.VideoCapture`）から取得した値を使用

#### 高優先度（パフォーマンス・安定性）
3. **特徴量抽出のメモリ対策**
   - `extract_video_features.py`で、一定フレームごとに`df.to_csv(mode='a')`するなどしてメモリを解放

4. **Loss関数の安定化**
   - `loss.py`の重み付けロジックを見直し、バッチ全体で統一的な平均化を実装

### 改善予定（優先度別）

#### ✅ 完了した改善
- ✅ **`otio_xml_generator.py`のリファクタリング**: デッドコードの削除完了
- ✅ **解像度の動的取得**: 入力動画から自動取得
- ✅ **特徴量抽出のメモリ対策**: Chunk処理によるメモリ解放
- ✅ **Loss関数の安定化**: 重み付けロジックの見直し
- ✅ **設定ファイル化**: 全てのマジックナンバーをYAMLファイルで設定可能に
- ✅ **予測値の平滑化**: Savitzky-Golayフィルタなどを実装
- ✅ **学習データの不均衡対策**: 自動分析とLoss重み付け調整
- ✅ **CLIP特徴量抽出の改善**: 線形補間により細かいシーン変化も捕捉
- ✅ **例外処理の強化**: MediaPipe初期化失敗時の適切なハンドリング
- ✅ **推論パラメータの自動最適化**: 学習時に検証データで最適値を計算
- ✅ **クリップレベルの評価指標**: 学習中にクリップ数・継続時間などを評価
- ✅ **動画単位のデータ分割**: データリーク防止のため、同じ動画のシーケンスを同じセットに配置
- ✅ **Focal Lossの導入**: 採用見逃しに重いペナルティを課してF1スコア向上
- ✅ **学習可視化**: リアルタイムグラフ表示（損失、精度、閾値、採用率など）

#### 高優先度（残り）
- [ ] **K-Fold Cross Validation**: 複数の分割で学習・評価して信頼性向上
- [ ] **テロップデコード**: Base64エンコードされたテロップをデコードして特徴量に含める
- [ ] **テロップのXML出力**: 学習したテロップ情報をXMLに出力する機能を実装
- [ ] **Asset ID管理の改善**: 特徴量ベースのマッチングまたは役割ベースのID管理
- [ ] **トラック配置改善**: 複数トラックに分散配置して編集しやすいXMLを生成

#### 中優先度（残り）
- [ ] **学習データの品質向上**: より多様な編集スタイルのデータを追加
- [ ] **XMLパーサーの強化**: ネストされたシーケンスやマルチカムクリップへの対応
- [ ] **特徴量抽出の高速化**: 並列処理の最適化
- [ ] **モデルの軽量化**: 推論速度の向上

#### 低優先度
- [ ] **ユニットテストの拡充**: カバレッジ向上
- [ ] **ドキュメントの充実化**: チュートリアルやFAQの追加
- [ ] **UIの追加**: GUIベースの設定・実行ツール

### 技術的負債

#### 解決済み
- ✅ **過剰なカット問題**: ギャップ結合と最小継続時間フィルタで対応
- ✅ **音声・動画の飛び飛び問題**: クリップフィルタリングで改善

#### 解決済み（機能面）
- ✅ **マジックナンバーの設定ファイル化**: 全てのパラメータをYAMLファイルで設定可能に
  - Active閾値、クリップフィルタリングパラメータなど
  - 学習時に最適値を自動計算して保存
  - 推論時に自動的にロード
- ✅ **予測値のジッター解消**: Savitzky-Golayフィルタなどで平滑化を実装
- ✅ **解像度の動的対応**: 入力動画から自動取得するように改善
- ✅ **モデルの確信度改善**: クラス不均衡の自動分析とLoss重み付け調整を実装

#### 未解決（機能面）
- **Base64形式の解析処理未実装**: Premiere ProのBase64エンコード形式の解析・デコード処理が必要
- **XMLパーサーの制限**: ネストされたシーケンスやマルチカムクリップに未対応
- **Asset ID管理の問題**: ファイル名ベースのID割り当てで汎用性がない
- **単一トラック配置の制限**: 複数トラック対応への改修が必要

#### 解決済み（コード品質）
- ✅ **デッドコードの削除**: `otio_xml_generator.py`のクリーンアップ完了
- ✅ **解像度の動的取得**: 入力動画から自動取得するように修正
- ✅ **メモリ最適化**: Chunk処理によるメモリ解放を実装（長時間動画でもOOMしない）
- ✅ **Loss関数の安定化**: バッチ全体で統一的な平均化を実装
- ✅ **CLIP特徴量の補間改善**: 線形補間により細かいシーン変化も捉えられるように改善
- ✅ **例外処理の強化**: MediaPipe初期化失敗時の適切なハンドリングを実装
- ✅ **クラス不均衡対策**: 自動分析とLoss重み付け調整を実装

#### 未解決（コード品質）
- **特徴量次元数の不一致**: コメントと実装が合致していない可能性（軽微）

### 設定ファイルによるカスタマイズ

推論パラメータは`configs/config_inference.yaml`で設定可能です：

```yaml
# クリップフィルタリング
clip_filtering:
  active_threshold: 0.29      # Active判定の閾値（学習時に自動最適化）
  min_clip_duration: 3.0      # 最小クリップ継続時間（秒）
  max_gap_duration: 2.0       # ギャップ結合の最大長（秒）
  target_duration: 90.0       # 目標合計時間（秒）
  max_duration: 150.0         # 最大合計時間（秒）

# 予測値の平滑化
smoothing:
  enabled: true               # 平滑化の有効/無効
  method: 'savgol'           # 手法: moving_average, savgol, ema
  window_size: 5             # ウィンドウサイズ
```

学習時の重み付けは`configs/config_multimodal_experiment.yaml`で設定：

```yaml
# クラス不均衡の自動調整
auto_balance_weights: true   # 自動的に最適な重みを計算

# Loss重み（auto_balance_weights=falseの場合に使用）
active_weight: 1.0
asset_weight: 1.0
scale_weight: 1.0
position_weight: 1.0
```

**想定される処理フロー**:
1. 10分（600秒）の動画を入力
2. モデルが重要なシーンを予測（Active確率）
3. ギャップ結合で短い不採用区間を埋める
4. 3秒未満のクリップを除外
5. 予測値を平滑化（ジッター軽減）
6. スコア（確信度）順に並べて上位を選択
7. 合計90秒（最大150秒）のハイライト動画を生成

## 🤝 貢献

プルリクエストを歓迎します！特に以下の分野での貢献を募集しています：
- 学習データの提供
- パフォーマンス最適化
- ドキュメントの改善
- バグ修正

## 📝 ライセンス

MIT License



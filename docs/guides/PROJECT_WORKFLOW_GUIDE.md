# 動画編集AI - プロジェクト全体の流れ

## 🎯 プロジェクトの目的
動画から自動的に編集パラメータ（カット位置、ズーム、クロップ、テロップ）を予測し、Premiere Pro用のXMLを生成する

---

## 📊 全体フロー図

```
┌─────────────────────────────────────────────────────────────────┐
│                        1. データ準備フェーズ                        │
└─────────────────────────────────────────────────────────────────┘
                                  ↓
    ┌──────────────────────────────────────────────────────┐
    │ 1-1. 編集済み動画 + Premiere Pro XMLを用意           │
    │      (editxml/フォルダ内)                            │
    └──────────────────────────────────────────────────────┘
                                  ↓
    ┌──────────────────────────────────────────────────────┐
    │ 1-2. XMLから編集パラメータを抽出                      │
    │      python premiere_xml_parser.py                   │
    │      → output_labels/*.csv                           │
    └──────────────────────────────────────────────────────┘
                                  ↓
    ┌──────────────────────────────────────────────────────┐
    │ 1-3. 動画から特徴量を抽出                            │
    │      python extract_video_features_parallel.py       │
    │      → input_features/*.csv                          │
    │      (音声、映像、テロップの特徴量)                   │
    └──────────────────────────────────────────────────────┘
                                  ↓
    ┌──────────────────────────────────────────────────────┐
    │ 1-4. 特徴量とラベルを統合                            │
    │      python data_preprocessing.py                    │
    │      → master_training_data.csv                      │
    └──────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│                        2. 学習フェーズ                            │
└─────────────────────────────────────────────────────────────────┘
                                  ↓
    ┌──────────────────────────────────────────────────────┐
    │ 2-1. マルチモーダルモデルの学習                       │
    │      python training.py --config config_multimodal.yaml│
    │      → checkpoints/best_model.pth                    │
    │      (音声 + 映像 + トラック特徴量を統合)              │
    └──────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────┐
│                        3. 推論フェーズ                            │
└─────────────────────────────────────────────────────────────────┘
                                  ↓
    ┌──────────────────────────────────────────────────────┐
    │ 3-1. 新しい動画から特徴量を抽出                       │
    │      (inference_pipeline.py内で自動実行)             │
    └──────────────────────────────────────────────────────┘
                                  ↓
    ┌──────────────────────────────────────────────────────┐
    │ 3-2. モデルで編集パラメータを予測                     │
    │      python inference_pipeline.py                    │
    │      → temp.xml (OTIO生成、音声カット済み)           │
    └──────────────────────────────────────────────────────┘
                                  ↓
    ┌──────────────────────────────────────────────────────┐
    │ 3-3. テロップをグラフィックに変換                     │
    │      python fix_telop_simple.py temp.xml final.xml   │
    │      → final.xml (Premiere Pro互換)                  │
    └──────────────────────────────────────────────────────┘
                                  ↓
    ┌──────────────────────────────────────────────────────┐
    │ 3-4. Premiere Proで開く                              │
    │      final.xmlをインポート                           │
    └──────────────────────────────────────────────────────┘
```

---

## 📝 詳細な手順

### フェーズ1: データ準備

#### ステップ1-1: 編集済み動画とXMLを用意
```
editxml/
├── video1.mp4
├── video1.xml  (Premiere ProからエクスポートしたXML)
├── video2.mp4
└── video2.xml
```

#### ステップ1-2: XMLから編集パラメータを抽出
```bash
python premiere_xml_parser.py
```
**出力**: `output_labels/video1_labels.csv`
- カット位置（start_frame, end_frame）
- スケール（scale）
- 位置（position_x, position_y）
- クロップ（crop_left, crop_right, crop_top, crop_bottom）

#### ステップ1-3: 動画から特徴量を抽出
```bash
python extract_video_features_parallel.py
```
**出力**: `input_features/video1_features.csv`

**抽出される特徴量**:
- **音声特徴量** (17次元):
  - audio_energy_rms: 音声エネルギー
  - audio_is_speaking: 発話検出
  - silence_duration_ms: 無音時間
  - text_is_active: 音声認識テキスト有無
  - telop_active: テロップ有無
  - speech_emb_0~5: 音声認識テキストの埋め込み
  - telop_emb_0~5: テロップテキストの埋め込み

- **映像特徴量** (522次元):
  - scene_change: シーン変化検出
  - visual_motion: 動き検出
  - saliency_x, saliency_y: 注目領域
  - face_count: 顔の数
  - face_center_x, face_center_y: 顔の中心位置
  - face_size: 顔のサイズ
  - face_mouth_open: 口の開き具合
  - face_eyebrow_raise: 眉の上がり具合
  - clip_0~511: CLIP視覚埋め込み

#### ステップ1-4: 特徴量とラベルを統合
```bash
python data_preprocessing.py
```
**出力**: `master_training_data.csv`
- 特徴量とラベルを時間軸で整列
- 学習用データセットとして保存

---

### フェーズ2: 学習

#### モデルの学習
```bash
python training.py --config config_multimodal.yaml
```

**モデル構造**:
```
┌─────────────────────────────────────────────────────────┐
│                  マルチモーダルモデル                      │
├─────────────────────────────────────────────────────────┤
│                                                         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐ │
│  │ 音声特徴量    │  │ 映像特徴量    │  │トラック特徴量 │ │
│  │  (17次元)    │  │  (522次元)   │  │  (240次元)   │ │
│  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘ │
│         │                 │                 │         │
│         ↓                 ↓                 ↓         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐ │
│  │ Audio Encoder│  │Visual Encoder│  │Track Encoder │ │
│  │  (128次元)   │  │  (256次元)   │  │  (256次元)   │ │
│  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘ │
│         │                 │                 │         │
│         └─────────┬───────┴─────────────────┘         │
│                   ↓                                   │
│         ┌──────────────────────┐                      │
│         │  Fusion Transformer  │                      │
│         │   (640次元 → 512次元) │                      │
│         └──────────┬───────────┘                      │
│                    ↓                                  │
│         ┌──────────────────────┐                      │
│         │   予測ヘッド           │                      │
│         │  - Active (2クラス)   │                      │
│         │  - Scale (1次元)      │                      │
│         │  - Position (2次元)   │                      │
│         │  - Crop (4次元)       │                      │
│         └──────────────────────┘                      │
└─────────────────────────────────────────────────────────┘
```

**出力**: `checkpoints_50epochs/best_model.pth`

---

### フェーズ3: 推論（新しい動画の自動編集）

#### ステップ3-1 & 3-2: 推論実行
```bash
python inference_pipeline.py \
    "D:\videos\new_video.mp4" \
    --model checkpoints_50epochs/best_model.pth \
    --output temp.xml
```

**処理内容**:
1. 動画から特徴量を自動抽出
2. モデルで編集パラメータを予測
3. OTIOを使用してXMLを生成
   - 映像と音声を同じ位置でカット
   - テロップに`[Telop]`マーカーを付与

**出力**: `temp.xml` (音声カット済み、テロップはマーカー付き)

#### ステップ3-3: テロップをグラフィックに変換
```bash
python fix_telop_simple.py temp.xml final.xml
```

**処理内容**:
- `[Telop]`マーカーを削除
- 最初のテロップ: 完全なグラフィックfile要素を生成
- 2番目以降: 最初のfile要素を参照

**出力**: `final.xml` (Premiere Pro互換)

#### ステップ3-4: Premiere Proで開く
1. Premiere Proを起動
2. ファイル → 読み込み → `final.xml`
3. 自動編集されたタイムラインが表示される！

---

## 🔧 主要なファイル

### データ準備
- `premiere_xml_parser.py` - XMLからラベル抽出
- `extract_video_features_parallel.py` - 動画から特徴量抽出
- `data_preprocessing.py` - データ統合

### モデル
- `model.py` - モデル定義
- `multimodal_modules.py` - マルチモーダルエンコーダー
- `multimodal_preprocessing.py` - 特徴量の正規化
- `feature_alignment.py` - 特徴量のアライメント

### 学習
- `training.py` - 学習スクリプト
- `multimodal_dataset.py` - データセット
- `loss.py` - 損失関数
- `config_multimodal.yaml` - 学習設定

### 推論
- `inference_pipeline.py` - 推論パイプライン
- `otio_xml_generator.py` - OTIO XML生成
- `fix_telop_simple.py` - テロップ変換

---

## 📊 データの流れ

```
編集済み動画 (MP4)
    ↓
[特徴量抽出]
    ↓
音声特徴量 (17次元) + 映像特徴量 (522次元)
    ↓
[モデル]
    ↓
編集パラメータ予測
    ↓
[XML生成]
    ↓
Premiere Pro XML
```

---

## 🎓 重要なポイント

### 1. マルチモーダル学習
- 音声、映像、トラックの3つのモダリティを統合
- Transformerで時系列パターンを学習
- 各モダリティの重要度を自動調整

### 2. 音声カット
- OTIOの`source_range`を使用
- 映像と音声を同じ時間範囲で追加
- 自動的に同じ位置でカット

### 3. テロップ処理
- 動画からOCRでテロップを抽出
- テキスト埋め込みで意味を学習
- Premiere Proのグラフィックとして出力

### 4. 予測の閾値調整
- active閾値: 0.29
- カット数を約500個程度に調整
- 閾値を変更することでカット数を調整可能

---

## 🚀 クイックスタート

### 新しい動画を編集する場合
```bash
# 1行で実行（推論 + テロップ変換）
python inference_pipeline.py "your_video.mp4" \
    --model checkpoints_50epochs/best_model.pth \
    --output temp.xml && \
python fix_telop_simple.py temp.xml final.xml

# Premiere Proで final.xml を開く
```

### 新しいデータで学習する場合
```bash
# 1. XMLからラベル抽出
python premiere_xml_parser.py

# 2. 動画から特徴量抽出
python extract_video_features_parallel.py

# 3. データ統合
python data_preprocessing.py

# 4. 学習
python training.py --config config_multimodal.yaml
```

---

## 📈 性能

- **学習データ**: 約10本の編集済み動画
- **学習時間**: 50エポック（約2-3時間、GPU使用時）
- **推論時間**: 約30秒/動画（特徴量抽出含む）
- **カット数**: 約500個/動画（閾値0.29の場合）

---

## 🔍 トラブルシューティング

### XMLが読み込めない
→ `fix_telop_simple.py`を実行したか確認

### カット数が多すぎる/少なすぎる
→ `inference_pipeline.py`の閾値を調整（現在0.29）

### 特徴量抽出が遅い
→ GPUを使用しているか確認（CUDA対応）

### モデルの精度が低い
→ 学習データを増やす、エポック数を増やす

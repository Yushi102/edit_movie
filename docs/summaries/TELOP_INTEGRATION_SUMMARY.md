# テロップ統合機能の実装完了

## 🎯 目的
Whisperの音声認識（話している内容）とテロップ（画面に表示される文字）の両方を学習に活用する

## ✅ 実装内容

### 1. テロップ抽出モジュール（`telop_extractor.py`）
- Premiere Pro XMLからエッセンシャルグラフィックスのテロップを抽出
- `<effect effectcategory="graphic">`の`<name>`タグから文字を取得
- 時系列データ（0.1秒刻み）に変換

**抽出例**:
- 「あっ」（1.41s - 2.05s）
- 「どこに落ちる要素があった？ｗ」（12.31s - 14.63s）
- 「バンッ」（25.61s - 26.25s）
- など

### 2. 特徴量抽出の更新（`extract_video_features_parallel.py`）
- `_extract_audio_features`関数にテロップ抽出機能を追加
- XMLファイルから自動的にテロップ情報を読み取り
- 音声特徴量CSVに`telop_active`と`telop_text`カラムを追加

### 3. テキスト埋め込みの拡張（`multimodal_dataset.py`）
- 音声認識テキスト（`text_word`）→ `speech_emb_0~5`（6次元）
- テロップテキスト（`telop_text`）→ `telop_emb_0~5`（6次元）
- 両方を別々の埋め込みとして処理

### 4. 特徴量アライメントの更新（`feature_alignment.py`）
- `telop_active`を離散特徴量として処理（Forward-fill補間）
- `telop_emb_*`を連続特徴量として処理（線形補間）
- 音声認識とテロップの埋め込みを区別

### 5. 設定ファイルの更新（`config_multimodal_experiment.yaml`）
```yaml
audio_features: 17  # 4 base + 1 telop_active + 6 speech + 6 telop
```

### 6. モデルの更新（`train.py`）
- `MultimodalTransformer`の`audio_features`を17次元に更新
- ログ出力に音声認識とテロップの埋め込み情報を追加

## 📊 新しい音声特徴量の構成（17次元）

### 基本特徴量（5次元）
1. `audio_energy_rms` - 音声エネルギー
2. `audio_is_speaking` - 話しているか
3. `silence_duration_ms` - 無音時間
4. `text_is_active` - 音声認識テキストが表示されているか
5. `telop_active` - テロップが表示されているか

### 音声認識テキスト埋め込み（6次元）
6-11. `speech_emb_0~5` - Whisperで認識した話している内容の埋め込み
   - 文字数
   - ひらがな有無
   - カタカナ有無
   - 漢字有無
   - 句読点有無
   - 正規化された長さ

### テロップテキスト埋め込み（6次元）
12-17. `telop_emb_0~5` - XMLから抽出したテロップの埋め込み
   - 文字数
   - ひらがな有無
   - カタカナ有無
   - 漢字有無
   - 句読点有無
   - 正規化された長さ

## 🔄 データフロー

```
動画ファイル
├─ 音声 → Whisper → 「ちょっとがんがない」
│   └─ speech_emb_0~5（話している内容）
│
└─ XML → テロップ抽出 → 「あっ」「どこに落ちる要素があった？ｗ」
    └─ telop_emb_0~5（画面に表示される文字）

↓ 統合

音声特徴量CSV（17次元）
├─ 基本特徴量（5次元）
├─ 音声認識埋め込み（6次元）
└─ テロップ埋め込み（6次元）

↓ モデルへ入力

MultimodalTransformer
├─ audio: (batch, seq_len, 17)
├─ visual: (batch, seq_len, 522)
└─ track: (batch, seq_len, 180)
```

## 🎯 学習できる情報

### 音声認識（Whisper）
- **話している内容**を学習
- 実際の発言内容
- 会話の流れ

### テロップ
- **編集者が意図的に配置した文字**を学習
- 強調したいポイント
- 視聴者の注目を集める場所
- 面白いリアクション

### 両方の組み合わせ
- 話している内容とテロップの関係
- どんな発言にどんなテロップを付けるか
- テロップのタイミング

## 🚀 次のステップ

### 1. 既存データの再抽出
既存の動画特徴量CSVにはテロップ情報が含まれていないため、再抽出が必要：

```bash
python extract_video_features_parallel.py --input_dir "path/to/videos" --output_dir input_features
```

### 2. トレーニング
新しい17次元の音声特徴量でモデルをトレーニング：

```bash
python train.py --config config_multimodal_experiment.yaml --enable_multimodal
```

## 💡 利点

1. **より豊かな情報**: 話している内容とテロップの両方を学習
2. **編集意図の理解**: テロップ配置から編集者の意図を学習
3. **柔軟な設計**: 音声認識とテロップを別々に処理できる
4. **後方互換性**: テロップ情報がない動画でも動作（ゼロベクトル）

## 📝 注意事項

- XMLファイルが存在しない動画は、テロップ情報がゼロベクトルになります
- エッセンシャルグラフィックスの名前（`<name>`タグ）をテロップテキストとして使用
- 暗号化されたテロップ内容（`value`タグ）は現在デコードしていません

## ✅ テスト結果

- ✅ テロップ抽出: 11個のテロップを正常に抽出
- ✅ 時系列変換: 551タイムステップ、185アクティブフレーム
- ✅ テキスト埋め込み: 6次元ベクトルに正常に変換
- ✅ CSV生成: 17次元の音声特徴量CSVを正常に生成

実装完了！🎉
